{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthKhandelwal/GAN/blob/main/GANs_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPSC 440 Assignment: Generative Adversarial Networks (GANs)"
      ],
      "metadata": {
        "id": "ooZ_3AAPonl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment will serve as a practical introduction to GANs by building a model to generate \"fake\" images of handwritten digits. We will use the infamous MNIST dataset."
      ],
      "metadata": {
        "id": "OLfsVAnQoEKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Consider setting `hardware accelerator` to `GPU` in `Runtime > Change runtime type` for faster learning. We will use PyTorch for modeling and `matplotlib` to display images."
      ],
      "metadata": {
        "id": "Xi743tABo_Nq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "GmjXhRnxl_Ct"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import os\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import IPython.display\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "generator = torch.Generator('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.set_default_device(device)\n",
        "# if torch.cuda.is_available():\n",
        "#   torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "# upload external file before import\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data loading step is already setup for you. The MNIST data set is normalized."
      ],
      "metadata": {
        "id": "qiAjVYUnoEzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
        "data_train = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
        "\n",
        "\n",
        "def emptyFolder():\n",
        "  for file in os.scandir(\"./generated_images\"):\n",
        "    if file.name.endswith(\".png\"):\n",
        "        os.unlink(file.path)\n",
        "# creating a directory (if not exists) to save epoch images, otherwise clear \n",
        "# the folder for any previous results\n",
        "if not os.path.exists(\"generated_images\"):\n",
        "    os.makedirs(\"generated_images\")\n",
        "else:\n",
        "  emptyFolder()"
      ],
      "metadata": {
        "id": "GopXBLi4ucWo"
      },
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. A GAN for MNIST\n",
        "You will build a GAN model that can generate convincing handwritten numbers."
      ],
      "metadata": {
        "id": "YH04mBWiKdBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.1) [1 point] Discover the dimensions of the dataset."
      ],
      "metadata": {
        "id": "MdMgOdmvvuMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Check the size of the inputs\n",
        "\n",
        "## BEGIN SOLUTION\n",
        "data_train.data.size()\n",
        "## END SOLUTION"
      ],
      "metadata": {
        "id": "m93zeJrQvqcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a0ce474-7b17-406f-edcd-dceb295b11fe"
      },
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will take the topdown approach, and implement the GAN model before implementing the inner adversarial networks."
      ],
      "metadata": {
        "id": "eoUq3edTREvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.2) [10 points] Configure the Generator"
      ],
      "metadata": {
        "id": "gHNm4c8a1-pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.loss import BCELoss\n",
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim,\n",
        "        learning_rate=0.01,\n",
        "        batch_size=100\n",
        "        ):\n",
        "      super().__init__()\n",
        "\n",
        "      self.learning_rate = learning_rate\n",
        "      self.batch_size = batch_size\n",
        "      self.in_dim = in_dim\n",
        "\n",
        "      # TODO: build a NN model with in_features=in_dim and out_features=in_dim\n",
        "      # For MNIST in_dim is 28 * 28 = 784. \n",
        "      # TODO: store the model in self.layers.\n",
        "      # HINT: Only use TanH activation at the end.\n",
        "      # ANOTHER HINT: In most cases, bigger networks are better networks.\n",
        "      \n",
        "      # self.layers = ...\n",
        "      # self.loss_func = ...\n",
        "      ## Begin Solution\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.Linear(in_dim, 256),\n",
        "          nn.LeakyReLU(0.2),\n",
        "          nn.Linear(256, 512),\n",
        "          nn.LeakyReLU(0.2),\n",
        "          nn.Linear(512, 1024),\n",
        "          nn.LeakyReLU(0.2),\n",
        "          nn.Linear(1024, in_dim),\n",
        "          nn.Tanh(),\n",
        "      )\n",
        "      self.loss_func = nn.BCELoss()\n",
        "      ## End Solution\n",
        "\n",
        "      self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "    \n",
        "    def forward(self, z):\n",
        "      # Use the layers you setup in __init__ for forward.\n",
        "       #z =  z.view(z.size(0), 1, 28, 28)\n",
        "       return self.layers(z).view(z.size(0), 1, 28, 28).to(device)\n",
        "\n",
        "    # Helper function to generate inputs for this generator\n",
        "    def generate_noise(self, batch_size):\n",
        "      return torch.randn(batch_size, self.in_dim)\n",
        "\n",
        "    def fit(self, discriminator_output):\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      # TODO: fill in the variable 'output' and 'loss' with the correct value\n",
        "      # loss = ...\n",
        "      ## BEGIN SOLUTION\n",
        "      loss = self.loss_func(discriminator_output, torch.ones(discriminator_output.shape))\n",
        "      ## END SOLUTION \n",
        "\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      return loss.item()"
      ],
      "metadata": {
        "id": "0Ql980zLw4TD"
      },
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.3) [9 points] Configure the Discriminator"
      ],
      "metadata": {
        "id": "EmVfWWlA4yva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim,\n",
        "        learning_rate=0.01,\n",
        "        batch_size=100,\n",
        "    ):\n",
        "      super().__init__()\n",
        "\n",
        "      self.learning_rate = learning_rate\n",
        "      self.batch_size = batch_size\n",
        "      self.in_dim = in_dim\n",
        "\n",
        "      # TODO: build an NN model with in_features=784 and out_features=1.\n",
        "      # HINT: What should the range of the last activation be?\n",
        "      # ANOTHER HINT: Again, bigger networks are usually better networks.\n",
        "\n",
        "      # self.layers = ...\n",
        "      # self.loss_func = ...\n",
        "      ## BEGIN SOLUTION\n",
        "      self.layers = nn.Sequential(\n",
        "            nn.Linear(in_dim, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "      self.loss_func = nn.BCELoss()\n",
        "      ## END SOLUTION\n",
        "\n",
        "      self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Flatten the input\n",
        "      x = x.view(x.size(0), self.in_dim).to(device)\n",
        "      return self.layers(x)\n",
        "\n",
        "    def fit(self, X_fake, X_real):\n",
        "      self.optimizer.zero_grad()\n",
        "      # TODO: train the discriminator using the X_fake parameter generated by \n",
        "      # the Generator and the X_real parameter representing the real images \n",
        "      # from MNIST dataset\n",
        "\n",
        "      output = ... # forward\n",
        "      loss = ... # compute loss\n",
        "      ## BEGIN SOLUTION\n",
        "      output = torch.cat((self.forward(X_fake), self.forward(X_real)))\n",
        "      loss = self.loss_func(\n",
        "            output, torch.cat((\n",
        "                torch.zeros((X_fake.shape[0], 1)),\n",
        "                torch.ones((X_real.shape[0], 1)))) \n",
        "      )\n",
        "      ## END SOLUTION\n",
        "      \n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      return loss.item()"
      ],
      "metadata": {
        "id": "P-ixQBuh0JdQ"
      },
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.4) [10 points] Implment the GAN using a Generator and a Discriminator. The skeleton code for Generator and Discriminator is availble below."
      ],
      "metadata": {
        "id": "NXWmkhAmvfxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerativeAdversarialNetwork(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_dim,\n",
        "               learning_rate=0.01,\n",
        "               epochs=5,\n",
        "               batch_size=100,\n",
        "               ):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.epochs = epochs\n",
        "\n",
        "    # TODO: create an assign the two adversarial nets using the arguments into the variable \n",
        "    # HINT: Refer to the classes below.\n",
        "\n",
        "    self.G = ... # generator\n",
        "    self.D = ... # discriminator\n",
        "\n",
        "    ## Begin Solution\n",
        "    self.G = Generator(in_dim, learning_rate=self.learning_rate, batch_size=self.batch_size).to(device)\n",
        "    self.D = Discriminator(in_dim, learning_rate=self.learning_rate, batch_size=self.batch_size).to(device)\n",
        "    ## End Solution\n",
        "\n",
        "    # noise to be used for testing the generator across training.\n",
        "    self.noise = self.G.generate_noise(24)\n",
        "\n",
        "  def fit(self, X, show_progress = True, download_trainig_image = False):\n",
        "    loader = DataLoader(dataset=X, batch_size=self.batch_size, shuffle=True, generator=generator)\n",
        "\n",
        "    X.data.to(device)\n",
        "    X.targets.to(device)\n",
        "    X.train_labels.to(device)\n",
        "\n",
        "    self.G.train()\n",
        "    self.D.train()\n",
        "\n",
        "    g_mean_losses = []\n",
        "    d_mean_losses = []\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "      g_total_loss = 0\n",
        "      d_total_loss = 0\n",
        "      for batch_index, (x, labels) in tqdm(enumerate(loader), total=int(len(X)/self.batch_size)):\n",
        "        # TODO: Update g_total_loss and d_total_loss\n",
        "        ...\n",
        "        ## Begin Solution\n",
        "        noise = self.G.generate_noise(self.batch_size)\n",
        "        generator_samples = self.G(noise).detach()\n",
        "        d_total_loss += self.D.fit(generator_samples, x)\n",
        "        discriminator_output = self.D(self.G(noise))\n",
        "        g_total_loss += self.G.fit(discriminator_output)\n",
        "        ## End Solution\n",
        "      # TODO: Compute the mean losses per image for this epoch.\n",
        "      # g_mean_loss = ...\n",
        "      # d_mean_loss = ...\n",
        "      ## Begin Solution\n",
        "      g_mean_loss = g_total_loss / self.batch_size\n",
        "      d_mean_loss = d_total_loss / self.batch_size\n",
        "      ## End Solution\n",
        "      g_mean_losses.append(g_mean_loss)\n",
        "      d_mean_losses.append(d_mean_loss)\n",
        "      if download_trainig_image:\n",
        "        # create the fake image for the epoch\n",
        "        generated_img = self.G(self.noise).cpu().detach()\n",
        "        # make the images as grid\n",
        "        generated_img = make_grid(generated_img)\n",
        "        # save the generated torch tensor models to disk\n",
        "        save_image(generated_img, f\"./generated_images/gen_img{epoch}.png\")\n",
        "      if show_progress:\n",
        "        print(f'Epoch: {epoch}/{self.epochs}, G mean loss: {g_mean_loss}, D mean loss: {d_mean_loss}')\n",
        "    \n",
        "    return [g_mean_losses, d_mean_losses]\n",
        "  \n",
        "  def forward(self, z):\n",
        "    z.to(device)\n",
        "    # TODO: Generate an image with the generator and return it.\n",
        "    ## Begin Solution\n",
        "    return self.G.forward(z)\n",
        "    ## End Solution\n"
      ],
      "metadata": {
        "id": "txdWWa9V9vAL"
      },
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.5) [3 points] Instantiate a `GenerativeAdversarialNetwork`, and train it, and graph the mean losses over epoch.\n"
      ],
      "metadata": {
        "id": "ka8hq76NWyyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ...\n",
        "g_mean_losses, d_mean_losses = [..., ...]\n",
        "\n",
        "## BEGIN SOLUTION\n",
        "model = GenerativeAdversarialNetwork(in_dim=28*28, epochs=50, learning_rate=0.0002, batch_size=100)\n",
        "model.to(device)\n",
        "g_mean_losses, d_mean_losses = model.fit(data_train, True, True)\n",
        "## END SOLUTION"
      ],
      "metadata": {
        "id": "cal7D0wDXlTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ec324d-a0d0-4822-ebdc-d46911b8d532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:28<00:00, 21.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/50, G mean loss: 30.85617324203253, D mean loss: 1.7904207569733261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 22.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50, G mean loss: 14.050667706131936, D mean loss: 3.1394958236813544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:28<00:00, 21.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/50, G mean loss: 12.847907614707946, D mean loss: 2.52256423920393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:28<00:00, 21.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3/50, G mean loss: 18.91187769293785, D mean loss: 1.8025971556454896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:31<00:00, 18.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4/50, G mean loss: 20.09537203788757, D mean loss: 1.4632513565570116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5/50, G mean loss: 16.75461251497269, D mean loss: 1.6426751981675625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6/50, G mean loss: 16.967262288331984, D mean loss: 1.6551528795808554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7/50, G mean loss: 15.136493203639985, D mean loss: 1.8627769979834556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:28<00:00, 21.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8/50, G mean loss: 13.705176984071732, D mean loss: 1.9981045046448707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9/50, G mean loss: 12.03156141757965, D mean loss: 2.247503556907177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/50, G mean loss: 11.087977709770202, D mean loss: 2.390969995111227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11/50, G mean loss: 11.073222463130952, D mean loss: 2.429570034146309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:28<00:00, 21.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12/50, G mean loss: 10.970747635364532, D mean loss: 2.507267704308033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13/50, G mean loss: 11.246565163731574, D mean loss: 2.4604398849606515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14/50, G mean loss: 11.439666665792465, D mean loss: 2.4406089921295644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15/50, G mean loss: 10.72113300204277, D mean loss: 2.5512314726412297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16/50, G mean loss: 10.96485235452652, D mean loss: 2.4203338894248008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:28<00:00, 21.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17/50, G mean loss: 11.237904928922653, D mean loss: 2.395324236601591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18/50, G mean loss: 9.991245486736297, D mean loss: 2.7182004007697107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19/50, G mean loss: 9.968625569343567, D mean loss: 2.636398644447327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20/50, G mean loss: 9.654106323719025, D mean loss: 2.821571376025677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21/50, G mean loss: 9.391401609778404, D mean loss: 2.7369176393747328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22/50, G mean loss: 9.272553932070732, D mean loss: 2.8136400523781777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23/50, G mean loss: 9.42173814535141, D mean loss: 2.769018847346306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:27<00:00, 21.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24/50, G mean loss: 9.001519318819046, D mean loss: 2.870273160934448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 465/600 [00:21<00:05, 23.40it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.6) [1 points] Plot the losses over epoch, and save it as an image.\n"
      ],
      "metadata": {
        "id": "mXy9TQiHh5Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## BEGIN SOLUTION\n",
        "plt.figure()\n",
        "plt.plot(g_mean_losses, label='Generator mean loss')\n",
        "plt.plot(d_mean_losses, label='Discriminator mean Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "## END SOLUTION"
      ],
      "metadata": {
        "id": "UtrJCQvyiJXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.7) [5 points] Generate an image with the model, and display."
      ],
      "metadata": {
        "id": "seh_GaT5jOWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = ...\n",
        "## Begin Solution\n",
        "image = model.forward(model.G.generate_noise(20)).cpu().detach()\n",
        "## End Solution\n",
        "grid_img = make_grid(image)\n",
        "save_image(grid_img, \"final_image.png\")\n",
        "plt.imshow(grid_img.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "JDAkKBWEjYrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Short Answer Questions"
      ],
      "metadata": {
        "id": "ckm3kZUDBwU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2.1) [2 points] What is the main difference between GANs and variational autoencoders (VAEs)?"
      ],
      "metadata": {
        "id": "ThOLK9A0B4Ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2.2) [3 points] What is a possible cause of the low output diversity of GANs?"
      ],
      "metadata": {
        "id": "DUYn_rimCDNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2.3) [2 points] Why can't pre-trained discriminators be used in GANs?"
      ],
      "metadata": {
        "id": "XRrM_9ruURxj"
      }
    }
  ]
}